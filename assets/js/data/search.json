[ { "title": "AWS Codepipeline Stuck at Cloudformation Prepare Stage", "url": "/aws-codepipeline-stuck-at-cloudformation-prepare-stage/", "categories": "AWS", "tags": "aws, cloud, codepipeline, cloudformation, cfn", "date": "2022-06-05 12:58:42 +0200", "snippet": "This issue puzzled me and even AWS support for weeks, although for the later it was mostly because of their availabilitynot their skills: I pushed in an update for one of my pipeline managed stacks but the Cloudformation prepare stage was not completing. Noerrors, nothing but it was displaying “running” without doing anything really. The Cloudformation change-setwas never created and after a day or two, it eventually timed out.As a standard course of action, I logged a support ticket, we went through the usual permission checks, our setup and soon but there was no clue so support escalated the ticket to backend engineering. After a couple of weeks of hassling ouraccount manager and support, we got the information we needed and fixed it.Partially, it was my fault but in the end I think AWS played a key role in this, hence we made a couple of requests toprevent this to happen in the future: Fix the service to include key backend information in CloudWatch Update the documentation BackgroundWe had our Elasticsearch service up for the Opensearch upgrade and it was managed with old, yaml style code and manualactions. As stated by the documentation,this change isn’t just clicking the upgrade button on console. There are specific steps to actually migrate the Cloudformation service objects into a new API compatible template and stack.Since we built a new Codepipeline driven automated deployment framework for CDK generated Cloudformation templates, wethought how convenient it is, we can move the service to a new template/stack management platform too.The migration is out of the scope of this article but basically, it included a step where the existing service objectessentially needs to be imported into a new stack then updated and so on.And this is where the process failed. It turns out, that Codepipeline does check the existing Cloudformation stack’s laststatus before proceeding: “AWS workers are not able to update the stack because within this operation we treat the key/value pair forstatus_status=IMPORT_COMPLETE as a state which cannot be updated.”This is utterly wrong to me, it is needless to build such a logic into other services, they should just let Cloudformationto do its job and decide what is valid to update and what is not.Updating a Cloudformation stack with IMPORT_COMPLETE last status is perfectly valid, but is is invalid for Codepipelineand to be frank, it is nonsense.Solution Update the stack manually (awscli, console, tool of your choice) with something, edit description, add a parameter,metadata or whatever you want, all we need is its status changed to UPDATE_COMPLETE.Once the stack is out of the IMPORT_COMPLETE state, Codepipeline will be be able to update the stack again." }, { "title": "macOS Optimised Battery Charging", "url": "/macOS-battery-health-and-optimised-charging/", "categories": "macOS", "tags": "macos, hardware", "date": "2022-04-25 22:33:18 +0200", "snippet": "I use my Macbook in clamshell mode most of the time. Back in the day with my old Retina Macbook, I disconnected mycharger couple of times a week to discharge the battery regularly. This simple practice enabled me to use my Macbookwith an external display while conditioning the battery for maximum cell life. My battery started failing around 680 cycles (health 86%) and eventually, it was time to replace it at 873 (health 70%).As for my newer Macbook Touch, it is a different story. I have a Belkin docking station nowadays and I really likedthe type-C thunderbolt option that feeds all my power, data, display, etc to my device with a single piece of cable.However, I am no longer be able to discharge my device regularly while in clamshell mode.BackgroundSince the arrival of the macOS release BigSur, optimised charging is built in to help your battery’s lifespan, but it was too good to be true according to myexperience.I noticed during the day, that my Macbook charged up to 80% then paused. The remaining 20% was finished later on theafternoon when the device was less utilised. But it required the device to be discharged first, which happens mostlyon weekends for me. My battery health declined fast, which means I was losing capacity. For about 110 cycles, mycapacity was around 92% compared the original design and it declined with a higher rate than my old Retina one.Luckily, that dodgy built-in keyboard failed (twice) on me, which resulted getting a brand new bottom case replacement(battery, keyboard, etc) as part of this repair program.To avoid getting my battery worn out fast again, I started searching for solutions and I think I found somethingpromising.SolutionI bumped into this Github project which enables pausingthe charging at any level you like. To be fair, it does not solve the problem described earlier, I am still unable todischarge my device while connected to the docking station. But at least, it allows the user to control the batterycharging to prevent overcharging while in clamshell mode. The bonus is that it is a simple, command line program availablevia brew, which can be scripted for further adjustments and management.$ brew tap zackelia/formulae$ brew install bclm$ sudo bclm write 77$ sudo bclm read 77$ sudo bclm OVERVIEW: Battery Charge Level Max (BCLM) Utility.USAGE: bclm &amp;lt;subcommand&amp;gt;OPTIONS: --version Show the version. -h, --help Show help information.SUBCOMMANDS: read Reads the BCLM value. write Writes a BCLM value. persist Persists bclm on reboot. unpersist Unpersists bclm on reboot. See &#39;bclm help &amp;lt;subcommand&amp;gt;&#39; for detailed help.As mentioned on the project’s page, sources [1][2][3]claim that the Li-Ion batteries are best between 40% and 80% charge levels so I limited my device to the recommended 77%.I have been running like this for some time and it seems, that the battery is performing well, capacity is stable. I cannot claim (yet), that it will prolong my battery’s life but it is worth to try." }, { "title": "AWS EKS StackSet created cluster access recovery", "url": "/aws-eks-stackset-cluster-access-recovery/", "categories": "AWS", "tags": "aws, cloud, eks, cloudformation, cfn, stackset", "date": "2019-08-16 14:23:34 +0200", "snippet": "Kubernetes is contagious and nowadays hard to ignore. So we decided to look into EKS to see how it would work forour microservice suite. As of today, there are 2 ways of creating (official) an EKS cluster: eksctl via CLI orpoint and click through the Web-UI.We have multiple accounts and use services in multiple regions, so I developed a custom CloudFormation template tobuild our EKS cluster with StackSets. While this worked perfectly, I found an issue of not being able to access thecluster at all.It turned out to be perfectly normal,since when an Amazon EKS cluster is created, the IAM entity (user or role) that creates the cluster is added to theKubernetes RBAC authorization table as the administrator (with system:master permissions). Initially, only that IAMuser (initiated the cluster creation) can make calls to the Kubernetes API server using kubectl.About StackSetsThe basics of StackSets,is that you create a role AWSCloudFormationStackSetAdministrationRole on your MASTER account (where the StackSetis launched from) and a role AWSCloudFormationStackSetExecutionRole on your TARGET account(s) where StackSetswill manage CloudFormation stacks for you.Assume role setupSo in this case, you need to make sure that your IAM user entity can assume (temporarily) the AWSCloudFormationStackSetExecutionRole.This can be done multiple ways, but the easiest would be to create a TEMP group, attach an inline policy to it and atcompletion, make your IAM user member of this TEMP group.The inline policy for your TEMP group:{ &quot;Version&quot;: &quot;2012-10-17&quot;, &quot;Statement&quot;: [ { &quot;Action&quot;: [ &quot;sts:AssumeRole&quot; ], &quot;Resource&quot;: [ &quot;arn:aws:iam::123456789100:role/AWSCloudFormationStackSetExecutionRole&quot;, ], &quot;Effect&quot;: &quot;Allow&quot; } ]}The &amp;lt;123456789100&amp;gt; number is YOUR target account ID where the EKS cluster is created.Create a TEMP AWS config entry:$ vim ~/.aws/config[profile eks]region = eu-central-1role_arn = arn:aws:iam::123456789100:role/AWSCloudFormationStackSetExecutionRolesource_profile = &amp;lt;your_normal_iam_profile_name&amp;gt;Test your assume role before proceeding:$ aws --profile eks sts assume-role --role-arn arn:aws:iam::123456789100:role/AWSCloudFormationStackSetExecutionRole --role-session-name testUpdate Kubernetes RBACGet the ConfigMap template from AWS:$ curl -o aws-auth-cm.yaml https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/aws-auth-cm.yamlEdit it:apiVersion: v1kind: ConfigMapmetadata: name: aws-auth namespace: kube-systemdata: mapRoles: | - rolearn: arn:aws:iam::123456789100:role/StackSet-EKS-c4c34dcd-45as-11-NodeInstanceRole-34F45DD54RSKK username: system:node:{{EC2PrivateDNSName}} groups: - system:bootstrappers - system:nodes - rolearn: arn:aws:iam::123456789100:role/Administrator username: admin groups: - system:masters The first element of the mapRoles is required for the worker nodes, so they can join the cluster automatically.For this, you need to get the ARN of the InstanceRole that is attached to your nodes, normally it is generated on the fly when the stack is created.The second element is our required admin access details. In my case a special role Administrator, that only certainusers can assume, but you could also add IAM user arn.When ready, get a copy of your cluster config with your new TEMP profile, then update the cluster:$ aws --profile eks --region eu-central-1 eks update-kubeconfig --name &amp;lt;eks cluster name you created&amp;gt;$ kubectl apply -f aws-auth-cm.yamlAt completion, delete the kube config you got with your TEMP profile, get a new one with your normalaws profile. Alternatively, edit the config and update the AWS_PROFILE key at the bottom…$ rm ~/.kube/config$ aws --profile &amp;lt;normal aws profile name&amp;gt; --region eu-central-1 eks update-kubeconfig --name &amp;lt;eks cluster name you created&amp;gt;From this point on, your normal IAM user profile should be able to access the cluster:$ kubectl get svc$ kubectl describe configmap -n kube-system aws-auth$ kubectl get namespace$ kubectl get nodes Do not forget to remove the TEMP group and the TEMP AWS config profile at completion." }, { "title": "AWS MFA enabled console with automated one time password", "url": "/aws-console-mfa-1password-otp-automation/", "categories": "macOS", "tags": "osx, macos, 1password, aws", "date": "2019-08-14 23:18:17 +0200", "snippet": "Moving to AWS is challenging and fun at the same time. Since our migration progresses well, we have enabled enforced MFAfor IAM accounts, that have Administrator access.With 1Password OTP, it is simple to setup and easy to use in the WebUI, but I felt there is room for an improvementfor the API calls, awscli commands over my console session so I don’t have to cut and paste every time my sessiontoken expires.First thing first, this requires macOS (OSX) operating system, bash, awscli, jq utilities and 1Password.I assume that, the reader is familiar with the AWS tools and services used here including but not limited to IAMaccess, API keys, session tokens and so on.AWS config$ vim .aws/credentials[mysession-mfa]aws_access_key_id = XXXXXXXXXXXXXXXXXXXXaws_secret_access_key = xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx[mysession]aws_access_key_id =aws_secret_access_key =mysession-mfa is the profile name for your IAM access, you should already have an API key and secret configured. Sinceour MFA is enforced (the policy used for that is out of the scope of this post), this is very limited. I can only changemy password, set MFA device and basics like that, so we only use it for MFA authentication, nothing else.mysession is the profile name, that will be MFA authenticated and once done, allow me to switch to my Administratorrole and become basically admin user of our account. This has NO key and secret configured, just a blank placeholder.System requirementsEnsure you have bash as your shell, have jq and at last but least have awscli installed. The name of your 1Passwordwallet entry, where you have the OTP configured is also crucial. I tend to add the profile name to the title (in brackets)so it is easy to find: &amp;lt;AWS IAM (mysession)&amp;gt;The bash function snippet$ vim ~/.bashrcaws_mfa_auth () { accountid=${1:-&quot;123456789000&quot;} loginid=${2:-&quot;first.last@company.com&quot;} profileid=${3:-&quot;mysession&quot;} printf &quot;Using account: $accountid\\n&quot; printf &quot;Using login: $loginid\\n&quot; printf &quot;Using profile: $profileid\\n&quot; AWS_MFA_ARN=&quot;arn:aws:iam::$accountid:mfa/$loginid&quot; AWS_MFA_SESSION_DURATION=&quot;28800&quot; osascript -e &quot;tell application \\&quot;System Events\\&quot; to tell process \\&quot;1Password mini\\&quot;&quot; \\ -e &quot;open location \\&quot;onepassword://extension/search/$profileid\\&quot;&quot; \\ -e &quot;end tell&quot; \\ -e &quot;delay 1&quot; \\ -e &quot;tell application \\&quot;System Events\\&quot; to tell process \\&quot;1Password mini\\&quot;&quot; \\ -e &quot;keystroke \\&quot;c\\&quot; using {command down, shift down, control down}&quot; \\ -e &quot;end tell&quot; \\ -e &quot;delay 1&quot; authcode=$(pbpaste) output=$(aws --profile ${profileid}-mfa sts get-session-token --serial-number ${AWS_MFA_ARN} --duration-seconds ${AWS_MFA_SESSION_DURATION} --token-code $authcode) AWS_ACCESS_KEY_ID=$(jq -r &#39;.Credentials.AccessKeyId&#39; &amp;lt;&amp;lt;&amp;lt; $output) AWS_SECRET_ACCESS_KEY=$(jq -r &#39;.Credentials.SecretAccessKey&#39; &amp;lt;&amp;lt;&amp;lt; $output) AWS_SESSION_TOKEN=$(jq -r &#39;.Credentials.SessionToken&#39; &amp;lt;&amp;lt;&amp;lt; $output) aws configure set aws_access_key_id ${AWS_ACCESS_KEY_ID} --profile $profileid aws configure set aws_secret_access_key ${AWS_SECRET_ACCESS_KEY} --profile $profileid aws configure set aws_session_token ${AWS_SESSION_TOKEN} --profile $profileid (sleep ${AWS_MFA_SESSION_DURATION} &amp;amp;&amp;amp; osascript -e &quot;display notification \\&quot;MFA token expired\\&quot; with title \\&quot;AWS $profileid\\&quot;&quot; &amp;amp;) exit 0 }As shown, the account, login and session IDs have defaults but overridable. I tend to set the session duration to8+ hours so I don’t have to authenticate multiple times a day especially when we are in heavy development.osascript takes care of fetching the OTP password from the correct wallet entry and at completion, awscli updatesthe placeholder entry with the temp API key and secret.The last part is bonus: a basic desktop notification that the session token expired.Finally, just watch it doing its thing…$ aws_mfa_authOnce the shell window closed, you could open another and try anything that you have permissions for BUT remember to useyour MFA session profile like this:$ aws ec2 describe-instances --region eu-central-1 --profile mysession --query &#39;Reservations[*].Instances[*].[InstanceId,Tags[?Key==`Name`].Value|[0],State.Name,PrivateIpAddress,PublicIpAddress]&#39; --output text | sort -k2 | column -tThis would list all running EC2 instances in a nice, column sorted window. Make sure your wallet is unlocked before running the function. This solution does not wait long for the wallet to beunlocked so if you are slow, it bombs out after few seconds." }, { "title": "Solving Poor Network Performance on RHEL and CentOS 7", "url": "/solving-poor-performance-on-rhel-and-centos-7/", "categories": "Network", "tags": "linux, network, java", "date": "2017-02-16 17:15:57 +0100", "snippet": "We are building the next generation online marketplace and part of it is a real-time Java application. This application is heavily optimised for its use case,handles zillions of short-lived tcp requests fast. Most of our operations complete quickly (&amp;lt;100ms) and some even more quickly (&amp;lt;500us).Our old application pool is based on CentOS 6 nodes and we are doing considerably well on them. However recently, we deployed our new CentOS 7 based serverfarm and for some reason, we have been unable to meet the expectations set by the old pool.We set these nodes identically, turned off CPU scaling, disabled THP, set IO elevators, etc. Everything looked fine, but the first impressions were that these newnodes have slightly higher load and deliver less.This was a bit frustrating.We got newer kernel, newer OS, newer drivers, everything is set the same way, but for some reason, we just could not squeeze the same amount of juice out of thesenew servers compared to the old ones.During our observation, we noticed that the interrupts on these new nodes were much higher (~30k+), compared to the old servers but I failed to follow upon it atthat point.Tuned performance profilesUp until recently, it did not occur to me that there are key differences in CentOS 7’s and CentOS 6’s latency-performance profile.Looking at the tuned documentation and the profiles, it looked like there is a more advanced profile available called network-latency, and it is a child profileof my current latency-performance profile.$ rpm -ql tuned | grep tuned.conf/etc/dbus-1/system.d/com.redhat.tuned.conf/etc/modprobe.d/tuned.conf/usr/lib/tmpfiles.d/tuned.conf/usr/lib/tuned/balanced/tuned.conf/usr/lib/tuned/desktop/tuned.conf/usr/lib/tuned/latency-performance/tuned.conf/usr/lib/tuned/network-latency/tuned.conf/usr/lib/tuned/network-throughput/tuned.conf/usr/lib/tuned/powersave/tuned.conf/usr/lib/tuned/throughput-performance/tuned.conf/usr/lib/tuned/virtual-guest/tuned.conf/usr/lib/tuned/virtual-host/tuned.conf/usr/share/man/man5/tuned.conf.5.gz$ grep . /usr/lib/tuned/network-latency/tuned.conf | grep -v \\#[main]summary=Optimize for deterministic performance at the cost of increased power consumption, focused on low latency network performanceinclude=latency-performance[vm]transparent_hugepages=never[sysctl]net.core.busy_read=50net.core.busy_poll=50net.ipv4.tcp_fastopen=3kernel.numa_balancing=0What it does additionally, is that it turns numa_balancing off, disables THP and makes some net.core kernel tweaks.After activating this profile, we can see the effect immediately.$ tuned-adm profile network-latency$ tuned-adm activeCurrent active profile: network-latencyMy interrupts drop from a whopping 70k down to 40k, which is exactly what my CentOS 6 interrupts measure during peak load.I could not stop there and I started experimenting with these settings and found, that the interrupts are directly affected by the numa_balancing kernel setting.Restoring numa_balancing puts the interrupt pressure back on the system immediately.ENABLE numa_balancing$ echo 1 &amp;gt; /proc/sys/kernel/numa_balancingDISABLE numa_balancing$ echo 0 &amp;gt; /proc/sys/kernel/numa_balancing Please note, that this profile is for a specific workload.Turning numa_balancing off may improve your interrupts but it can degrade your application some other way.Experiement with the available profiles and stick to the one that gives you the best results." }, { "title": "Conditional SNAT with iRule on F5", "url": "/conditional-snat-with-irule-on-f5/", "categories": "Network", "tags": "f5, irule, network", "date": "2015-09-14 23:03:35 +0200", "snippet": "Quick and dirty guide about how to create conditional SNAT with iRule on F5 and rewrite (NAT) IP addresses based on specific conditions.We have 2 public IP netblocks for our production network, one is geographically registered in LA, California, the other is Amsterdam, Netherlands. It is very common thatservices such as Google, Amazon, Akamai, etc serve requests based on their source but occasionally they get it wrong so I needed a way to control what netblock my requestis addressed out of.Furthermore, some of our services require one-to-one IP mappings so I had to come up with a solution that solves the following: check if the destination address is on the target list check if the source of the request has one-to-one mapping for outgoing IP if it does and the destination is on our target list then rewrite the address to the matched map if it does not have one-to-one map and the the destination is on our target list then rewrite the address to the default NAT address otherwise do nothing, send packet out with its original source addressltm data-group internal snat_for_destination { records { 8.8.8.8/32 { data googledns } } type ip}What matters here is the key (IP), the value (googledns) is just a comment although you can use it in logging statement if you want.ltm data-group internal snat_dmz_to_wan_map { records { 1.2.3.4/32 { data 4.3.2.1 } } type ip}ltm rule snat_dmz_to_wan_map { when CLIENT_ACCEPTED { # https://devcentral.f5.com/wiki/iRules.class.ashx?lc=1 # check IF the request&#39;s destination IP is in given match list if { [class match [IP::local_addr] equals snat_for_destination]} { # pick IP based 1-to-1 SNAT mapping for connecting client from given list if { [class match [IP::client_addr] equals snat_dmz_to_wan_map]} { # set variable for matched address object set snat_addr [class match -value [IP::client_addr] equals snat_dmz_to_wan_map] log local0. &quot;Connection from [IP::client_addr] to [IP::local_addr] rewrite as $snat_addr \\[iSNAT\\]&quot; snat $snat_addr } else { # on failed map lookup, default to F5&#39;s floating address log local0. &quot;Connection from [IP::client_addr] to [IP::local_addr] automap as 5.6.7.8 \\[iSNAT\\]&quot; snat automap } } else { forward }}Now, the only thing we need to do is to add this resource to the required Virtual Server object." }, { "title": "Solving OpenVPN poor throughput and packet loss", "url": "/solving-openvpn-poor-throughput-and-packet-loss/", "categories": "Network", "tags": "network, openvpn, security", "date": "2015-06-06 14:27:29 +0200", "snippet": "This not about optimising OpenVPN, it is about solving OpenVPN poor throughput and packet loss issue, where the server receives traffic faster than it actually process.We are currently in the process of moving data centers. This requires our Couchbase data to be in sync between Gütersloh (DE) and AMS-IX (NL) which does mean that XDCR needsto pump few hundred Gigs across every day and fast. After about 20 minutes or so, everything started to slow down for an unknown reason.Our choice for VPN solution was OpenVPN due to some limitations caused by the managed network at the german side, so we built the tunnel and managed to get a reasonable linkwith ~20ms TTL, initial throughput tests showed:[ 4] 0.0-30.1 sec 170 MBytes 47.5 Mbits/secIt was not so flash but I did not suspect anything at that point, I accepted it as the capability of the tunnel. Further investigation however revealed TX packet drops on thetunnel interface:tun1 Link encap:UNSPEC HWaddr 00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00 inet addr:10.0.0.129 P-t-P:10.0.0.130 Mask:255.255.255.255 UP POINTOPOINT RUNNING NOARP MULTICAST MTU:1500 Metric:1 RX packets:7409196373 errors:0 dropped:0 overruns:0 frame:0 TX packets:6084526548 errors:0 dropped:2642021 overruns:0 carrier:0 collisions:0 txqueuelen:100 RX bytes:941878733670 (877.1 GiB) TX bytes:3395805058712 (3.0 TiB)It seemed that the tunnel is not being able to keep up with the amount of traffic it received. After some reading, it turned out, that OpenVPN setstxqueuelen parameter to 100 as default for the tunnel interfaces on both, client and server. It is essentiallya buffer, and managed by the network scheduler.The solution was to set this to 1000, identical to the physical interface configurations:$ grep txqueuelen /etc/openvpn/server-udp.conftxqueuelen 1000After restarting OpenVPN on both, server and client side, there was no packet drop on the tunnel interfaces and the throughput was better too:[  4]  0.0-30.3 sec   267 MBytes  73.9 Mbits/secFor further optimisation, visit the official Linux guide." }, { "title": "Merging Pillars in SaltStack", "url": "/merging-pillars-in-saltstack/", "categories": "Python", "tags": "python, salt", "date": "2014-08-16 23:26:41 +0200", "snippet": "Merging or joining Pillars in SaltStack is supported but somewhat limited. It took me sometime to work out a clean solution to support a specific manipulation so to make this easier, I am going to share my real life example.Merging Pillars in SaltStackI wrote a reasonably complex formula to manage our DNS (bind9) servers including zone files. As a common approach, I decided to use Pillar for configuration to make the formulageneric and reusable.My formula required the following Pillar data (YAML):bind: config: user: root group: named mode: 640 custom: allow-query: - 127.0.0.1 - 192.168.1.0/24 allow-transfer: - 127.0.0.1 - 192.168.1.0/24 recursion: &quot;yes&quot; zone-statistics: &quot;yes&quot; transfer-format: many-answers interface-interval: 0 zones: domain.com: config: type: master file: domain.com.hosts also-notify: - 192.168.1.1 - 192.168.1.2 soa: ttl: 43200 ; 12 hours email: hostmaster.domain.com. refresh: 10800 ; 3h refresh retry: 3600 ; 1h retry expire: 604800 ; 1w expire minimum: 10800 ; 3h minimum ns: - ns1.domain.com. - ns6.otherdomain.net. - ns7.other.sub.domain.org. sub.domain.org: config: type: slave file: slaves/sub.domain.org.hosts masters: - 192.168.1.1 - 192.168.1.2 sub.domain.net: config: type: forward forward: only forwarders: - 172.16.1.1To reuse my formula, I needed slightly different pillar for each DNS server but I wanted to reuse existing details to avoid duplication and pollution hence I ended up splittingthe pillar into few files:/srv/salt/pillar/base/bind/named.sls/srv/salt/pillar/base/bind/zones-master.sls/srv/salt/pillar/base/bind/zones-slave.sls/srv/salt/pillar/base/bind/zones-other.slsThis allows the use or import of the named.sls (core config) for every server and depending on “role” even additional zones as required:/srv/salt/pillar/base/bind/named.slsbind: config: custom: ... .../srv/salt/pillar/base/bind/zones-master.slsbind: zones: domain.com: ... ...First, I tried adding init.sls with the ”include” statement:include: - bind.named{%- if grains[&#39;dnsrole&#39;] == &#39;master&#39; %} - bind.zones-master - bind.zones-other{%- endif %}This did not work as expected, actually it overrides either the first subkey of the named.sls (custom) or the zone file (zone) depending on which gets read first during compile.I could have used the include statement with the nesting but it only works if I restructurethe zone files and remove nesting from the bind key as well as include one zone.One option was to move this logic into the top.sls:base: &#39;roles:dns&#39;: - match: grain - bind.named - bind.zones-masterThis worked perfectly, my pillars were merged exactly the I way I wanted. However, I did not want to move a messy if-else logic there to target master/slave servers differently.Adding the if-else logic to the zone files looked not so ideal, changing the grain structure for better targeting seemed also just “too much” for what I needed to accomplish.Template Engine to the RescueMy solution was hiding here, I just had to compose the recipemyself. This method can be used in state files as well giving you the much needed power of code reuse./srv/salt/pillar/base/bind/init.sls{%- import_yaml &#39;bind/named.sls&#39; as named with context -%}{%- import_yaml &#39;bind/zones-master.sls&#39; as masters with context -%}{%- import_yaml &#39;bind/zones-slave.sls&#39; as slaves with context -%}{%- import_yaml &#39;bind/zones-other.sls&#39; as others with context -%}{%- if grains[&#39;dnsrole&#39;] == &#39;master&#39; %}{%- do named.bind.update(masters.bind) %}{%- elif grains[&#39;dnsrole&#39;] == &#39;slave&#39; %}{%- do named.bind.update(slaves.bind) %}{%- endif %}{%- do named.bind.zones.update(others.bind.zones) %}{{ named }}This imports all YAML files, compiles them as dictionaries and basically makes them available as objects by the given name. Then we just use the power of Jijna2 and carry outa dictionary update on specific objects from a specific nested key. To me it is readable, centralised and keeps the control inside the bind pillar, yet giving the power of flexibility.Minion targeting is now easy, keeping the top.sls clean and readable as possible:base: &#39;roles:dns&#39;: - match: grain - bind" }, { "title": "WD MyCloud 2T NAS Review", "url": "/wd-mycloud-2t-nas-review/", "categories": "Hardware", "tags": "hardware, nas, wd", "date": "2014-08-04 00:38:52 +0200", "snippet": "I purchased a Seagate Central 2T NAS 5 monthsago, for a low cost home media center solution. It worked reasonably well considering the low ~US130 cost although, I had ongoing issues with firmware updates, occasional driveperformance, etc. Unfortunately, it failed last week and while I was looking for alternatives, I learnt that I was not the only one having problems with that device so I simplylost trust in Seagate forever.I returned the drive, and the store offered me the WD MyCloud 2T as a replacement alternative without extra cost whatI happily accepted.WD MyCloud 2T NAS ReviewThis is my personal opinion and experience compared to Seagate Central, not necessarily an official “review” of the hardware and its provided features.DesignThe enclosure is very solid, fairly compact and feels good. I find the upright design a bit impractical as well as unstable compared to the Seagate Central and I have toadmit, I am not a big fan of the white housing and the blue LED in front but it’s just personal preference. Comparing to Seagate Central, the device feels actually a lotcooler, it’s more like hand warm rather than hot.Features: 1 x Gbit interface 1 x USB 3.0 port Dual Core processor DLNA 1.5 and UPnP Certified iTunes support TimeMachine support Mobile apps Remote accessThe GOOD:Software:The WebUI is very nice, clean and both easy to read and drive. The front page is well organised, dashboard alike but not “too busy” and easy to understand even for an averageuser but still giving you the option for detailed information, should you need it.Shares:Shares can have individual access settings or set public (accessible without password) as well as set to be excluded from media serving. (does NOT include exclusion from mediascan unfortunately)Notifications:You can set up email notifications and their importance. It’s not so much of a monitoring but certainly helps to keep you up to date about the events occurring on the driveincluding predictive failure, reboots, etc.Settings:Not so much of a feature, but it was great to see (after Seagate Central) that I have the ability to backup my configuration. Unfortunately, it only backs up the basics likenetwork settings but it does not save some of the personal preferences such as “Cloud Access” settings, etc.Network:One of the biggest surprise (positive) for me was the fact that this device allows SSH access as well as FTP access. Furthermore, the access is root which may not be good forsome but advanced users can really leverage this functionality especially when disaster strikes.Safepoints:Basically, you can set up another device and create a snapshot or mirror of your data at any given point of time. Considering the sizes available, I am not sure how feasiblethis is or how long it takes to copy 2T-4T over the cable even on Gbit network.Having the ability to clone one drive to another is not a bad idea and if you are looking for safety and value, it is cheaper to get 2 of these than oneMyCloud EX with 2 disks. Although that would give you encryption support as well as continuos operation even when one of the drive fails(assuming RAID1 setup).DLNA 1.5 and Twonky:Both my Samsung BluRay player and TV can stream seamlessly from the device. Just finished updating the firmware and was delighted to see a fairly recent Twonky release 7.2.8(this time of writing) included in the image.Under the hood:It’s running Debian Linux 7, data volumes feature EXT4 filesystem on dual core ARM platform.Linux WDMyCloud 3.2.26 #1 SMP Tue Jun 17 15:53:22 PDT 2014 wd-2.2-rel armv7lThe programs included with the Debian GNU/Linux system are free software;the exact distribution terms for each program are described in theindividual files in /usr/share/doc/*/copyright.Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extentpermitted by applicable law.WDMyCloud:~# cat /proc/cpuinfoProcessor : ARMv7 Processor rev 1 (v7l)processor : 0BogoMIPS : 1292.69processor : 1BogoMIPS : 1292.69Features : swp half thumb fastmult vfp edsp neon vfpv3 tlsCPU implementer : 0x41CPU architecture: 7CPU variant : 0x2CPU part : 0xc09CPU revision : 1Hardware : Comcerto 2000 EVMRevision : 0001Serial : 0000000000000000WDMyCloud:~# free total used free shared buffers cachedMem: 232448 170816 61632 0 7680 38784-/+ buffers/cache: 124352 108096Swap: 500672 94208 406464WDMyCloud:~# mount/dev/root on / type ext3 (rw,relatime,errors=continue,barrier=1,data=ordered)tmpfs on /run type tmpfs (rw,nosuid,noexec,relatime,size=23296k,mode=755)tmpfs on /run/lock type tmpfs (rw,nosuid,nodev,noexec,relatime,size=40960k)proc on /proc type proc (rw,nosuid,nodev,noexec,relatime)sysfs on /sys type sysfs (rw,nosuid,nodev,noexec,relatime)tmpfs on /dev type tmpfs (rw,relatime,size=10240k,mode=755)tmpfs on /run/shm type tmpfs (rw,nosuid,nodev,noexec,relatime,size=5120k)devpts on /dev/pts type devpts (rw,nosuid,noexec,relatime,gid=5,mode=620)fusectl on /sys/fs/fuse/connections type fusectl (rw,relatime)tmpfs on /tmp type tmpfs (rw,relatime,size=102400k)/dev/root on /var/log.hdd type ext3 (rw,relatime,errors=continue,barrier=1,data=ordered)ramlog-tmpfs on /var/log type tmpfs (rw,relatime,size=20480k)/dev/sda4 on /DataVolume type ext4 (rw,noatime,nodiratime,user_xattr,barrier=0,data=writeback)/dev/sda4 on /CacheVolume type ext4 (rw,noatime,nodiratime,user_xattr,barrier=0,data=writeback)/dev/sda4 on /nfs/TimeMachineBackup type ext4 (rw,noatime,nodiratime,user_xattr,barrier=0,data=writeback)/dev/sda4 on /nfs/Public type ext4 (rw,noatime,nodiratime,user_xattr,barrier=0,data=writeback)/dev/sda4 on /nfs/SmartWare type ext4 (rw,noatime,nodiratime,user_xattr,barrier=0,data=writeback)/dev/sda4 on /nfs/vari type ext4 (rw,noatime,nodiratime,user_xattr,barrier=0,data=writeback)nfsd on /proc/fs/nfsd type nfsd (rw,relatime)The BAD:Remote Access:I never liked this idea, never meant to use it but you can access your content over the internet after registering your user on wdmycloud.com.I was wondering about why it is and how my content is made available, whether it’s encrypted, etc. After logging in via SSH, I discovered a process running on the device:root 1585 2.0 1.5 6464 3712 ? SN 07:48 0:00 openvpn /usr/local/orion/openvpnclient/client.ovpnWhen you disable “Cloud Access” under “Settings -&amp;gt; General”, this process disappears so generally, I was happy to see that personal content is served encrypted over the Internet.However, I see some serious privacy issues around using OpenVPN for this feature.How it is configured:WDMyCloud:~# grep . /usr/local/orion/openvpnclient/client.ovpn | grep -vE &#39;;|#&#39;clientdev tunproto udpremote orionrelaya37.wd2go.com 14187remote orionrelaya42.wd2go.com 14418remote orionrelaya13.wd2go.com 14508remote orionrelaya52.wd2go.com 14603remote orionrelaya1.wd2go.com 14299remote orionrelaya67.wd2go.com 14789remote orionrelaya16.wd2go.com 14849remote orionrelaya15.wd2go.com 14264remote orionrelaya56.wd2go.com 14104remote orionrelaya24.wd2go.com 14441remote orionrelaya38.wd2go.com 14711remote orionrelaya4.wd2go.com 14740remote orionrelaya63.wd2go.com 14775remote orionrelaya74.wd2go.com 14935remote orionrelaya96.wd2go.com 14097remote orionrelaya97.wd2go.com 14807remote orionrelaya70.wd2go.com 14592remote orionrelaya65.wd2go.com 14777remote orionrelaya26.wd2go.com 14076remote orionrelaya79.wd2go.com 14258remote orionrelaya25.wd2go.com 14080remote orionrelaya62.wd2go.com 14285remote orionrelaya6.wd2go.com 14198remote orionrelaya17.wd2go.com 14450remote orionrelaya18.wd2go.com 14703remote orionrelaya75.wd2go.com 14405remote orionrelaya51.wd2go.com 14442remote orionrelaya28.wd2go.com 14474remote orionrelaya88.wd2go.com 14041remote orionrelaya21.wd2go.com 14080remote orionrelaya35.wd2go.com 14174remote orionrelaya66.wd2go.com 14824remote orionrelaya49.wd2go.com 14745remote orionrelaya73.wd2go.com 14091remote orionrelaya53.wd2go.com 14688remote orionrelaya61.wd2go.com 14884remote orionrelaya47.wd2go.com 14858remote orionrelaya32.wd2go.com 14785remote orionrelaya76.wd2go.com 14217remote orionrelaya41.wd2go.com 14183remote orionrelaya72.wd2go.com 14173remote orionrelaya78.wd2go.com 14709remote orionrelaya7.wd2go.com 14140remote orionrelaya98.wd2go.com 14744remote orionrelaya22.wd2go.com 14257remote orionrelaya64.wd2go.com 14021remote orionrelaya69.wd2go.com 14003remote orionrelaya48.wd2go.com 14064remote orionrelaya71.wd2go.com 14951remote orionrelaya94.wd2go.com 14567tls-exitexplicit-exit-notify 3script-security 2up &quot;/bin/rm -f /var/log/messages &amp;amp;&amp;amp;&quot;echo &quot;Initialization Sequence Completed&quot;remote-randomresolv-retry infinitenobindpersist-keypersist-tunca /usr/local/orion/openvpnclient/ca.crtns-cert-type serververb 3sndbuf 262144--auth-user-pass /usr/local/orion/openvpnclient/auth.txtreneg-sec 0In nutshell: when you enable “Cloud Access”, you start a VPN client which “connects” to one of those “orion” WD endpoints you see configured as “remote” and build a point-to-pointencrypted tunnel with it. This brings up a new virtual “tun” interface on your device with a specific class A private IP address, in my case: tun0 Link encap:UNSPEC HWaddr 00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00 inet addr:10.8.65.85 P-t-P:10.8.0.1 Mask:255.255.255.255 UP POINTOPOINT RUNNING NOARP MULTICAST MTU:1500 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:100 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B)This basically exposes your ENTIRE device to the “other” end you established the VPN tunnel with. The system is a cut-down version of Linux, it does not have firewall filteringor other security software built into it. Considering that FTP and SSH running on your device, anybody from the other end of the tunnel has the ability to log into your deviceand do whatever they want, including browsing your content.While I understand that it is a cost effective way of protecting your own content from 3rd parties, it does not protect you from the vendor itself so if you want to get accessto your content while away, you will have to trust WD some ways.AFP performance:Bad news for OSX users, even listing a decent directory with few hundred files takes a long time. I could not be bothered too much about it,OSX Mavericks now defaults to SMB so Apple clearly signaled that it’s moving away from its in-house developed protocol.Using the device over SMB (cifs) is fine, although I have not upgraded to OSX Mavericks yet so my experience is based on OSX Mountain Lion 10.8.5.Thumbnails and the mysterious .wdmc directories:If you log into the device via SSH or browse it with AFP, you will see mysterious .wdmc folders all over the place with lots of tiny image files in them. They are thumbnailsof your media files so every folder that has media (image, video) in it will have one of these.SMB (samba) hides these so you won’t see these normally but the issue with this service is that they are created regardless you want them or not. I am an amateur photographer,have thousands of images and these are not only take space from the device, but also expensive to create and manage. When you upload new content such as image, the device willdisplay “Content Scan” “Building”, that means it’s scanning and polluting your drive with these and depending on the volume, it can take a long long time and your drive performancewill be degraded.I was under the impression, that if I turn media serving off for my “photo share” it would be ignored. I actually like these thumbnails for my video share” what I browse overDLNA but I don’t want it for everything.So until WD kindly gives us the WebUI option to turn this off, you can do this (advanced users only):$ /etc/init.d/wdmcserverd stop$ /etc/init.d/wdphotodbmergerd stop$ update-rc.d wdphotodbmergerd disable$ update-rc.d wdmcserverd disableThat will just disable the service, it will not remove the directories and this most likely to be required again after firmware updates." }, { "title": "Load Balancing and Sticky Sessions by URL Parameter", "url": "/load-balancing-and-sticky-sessions-by-url-parameter/", "categories": "Network", "tags": "apache, nginx, proxy", "date": "2014-07-29 18:45:02 +0200", "snippet": "To be able to mimic our production workload in testing, we had to come with a low cost solution to load balance HTTP traffic between few application servers. In addition tothat, for the first (initial request) we required even distribution amongst the backend nodes but, subsequent requests needed to be handled by the same backend server.This task was relatively easy with NGINX, our preferred HTTP server however lately, I had to come up with a solution forapache 2.2 which was not as straight forward.Load Balancing and Sticky Sessions by URL Parameterapache maintains one of the best online documentation for most of its versions so finding a solution to my problem was not hard at all. However,most solutions are based on cookies what I could not use. The issue withcookies is that I have limited control over them, there is no guarantee that the user has cookies enabled, or it will be saved not to mention that cookies can become corrupted too.The other issue with cookie based load balancing is that once you have one set, you always ended up on the same backend server even when you make your first request.Our requirement was that for initial calls, we want randomly allocated backend then for subsequent calls we want the user to be handled by the same server what handled thefirst call.Sticky sessions with URL parameter is supported by apache’s mod_proxy:The second way of implementing stickyness is URL encoding. The web server searches for a query parameter in the URL of the request. The name of the parameter is specifiedagain using stickysession. The value of the parameter is used to lookup a member worker with route equal to that value.For some reason, I had mixed results, over all it seemed unreliable hence I had to come up with my own solution based on mod_rewrite:RewriteLog logs/virtualhost_rewrite.logRewriteLogLevel 2RewriteEngine OnRewriteCond %{QUERY_STRING} (myparam=myvalue1)RewriteRule (.*) http://lb-node1.domain.com:8080%{REQUEST_URI} [P,L]RewriteCond %{QUERY_STRING} (myparam=myvalue2)RewriteRule (.*) http://lb-node2.domain.com:8080%{REQUEST_URI} [P,L]ProxyErrorOverride On&amp;lt;Proxy balancer://loadbalancer&amp;gt; BalancerMember http://lb-node1.domain.com:8080 BalancerMember http://lb-node2.domain.com:8080&amp;lt;/Proxy&amp;gt;ProxyPass / balancer://loadbalancer/ProxyPassReverse / http://lb-node1.domain.com:8080/ProxyPassReverse / http://lb-node2.domain.com:8080/ Important to point out, that mod_rewrite rules are evaluated first before any other settings or rules and this behaviour is critical for this functionality.We basically check the incoming URL and if we a have match on our URL parameter, we pass the request handling to mod_proxy without evaluating any other rules. Our URL parametersare always unique and set by the backend servers so we need as much rewrite rules as backed servers we have.The &amp;lt;Proxy&amp;gt; directive sets up a standard load balancer, this ensures that initial request can be load balanced evenly between all member servers. The ProxyPassReversedirectives ensure that we correctly pass redirects back to the clients…The only issue with this setup is that, if one of the application servers become unreachable, the clients will receive 502 (bad gateway) if they have URL parameter set.NGINX is much easier to set up, and since it monitors the backend servers, it can proxy traffic to other available backend servers regardless of the URL parameter being set or not.upstream lb-pool { server lb-node1.domain.com:8080; server lb-node2.domain.com:8080;}upstream node1 { server lb-node1.domain.com:8080; server lb-node2.domain.com:8080 backup;}upstream node2 { server lb-node1.domain.com:8080 backup; server lb-node2.domain.com:8080;}server { listen 80; location / { proxy_cache off; proxy_pass_header off; proxy_set_header Host $http_host; if ($arg_myparam = &quot;myvalue1&quot;) { proxy_pass http://node1; break; } if ($arg_myparam = &quot;myvalue2&quot;) { proxy_pass http://node2; break; } proxy_pass http://lb-pool;}" }, { "title": "Dashlane vs 1Password", "url": "/dashlane-vs-1password/", "categories": "Security", "tags": "password, security", "date": "2014-07-17 00:33:29 +0200", "snippet": "I am a sysop / devops engineer, love open source and security so I tend to ignore commercial software. For password valet, I have been using KeePass for years and happy with it except a couple of things: written in .NET so cross platform integration has its challenges browser integrationAlthough the browser integration is reasonably good now on Windows, it’s not as refined as its commercial competitors such as Dashlane or1Password. So I decided to investigate these utilities to see if they can convince me to switch.Update - July 28th, 2014:I have to admit, that most of my concerns around 1Password were related to the earlier Windows version. Since my evaluation,1Password 4 was made available for Windows and looks, feels, behaves exactlylike on OSX.I am very happy to say that, I made the switch and I am a satisfied 1Password customer ever since. In the end, I decided not to look at cost when it comes to security,and it seems that I made the right decision.Dashlane vs 1PasswordI didn’t want to go into detail and compare the core or advanced features of these apps, you can find elsewhere. My goal was to find something more convenient and easier touse than KeePass but keeping the balance between security, functionality and convenience.It was also important to me how trusted the vendor is, what kind of reputation they may have and how easy it is to get my data, should they disappear from the market.This is purely my gut feeling / opinion and based on my quick and dirty trial based on versions available at this time of writing. I looked at how they compare to my currentsoftware KeePass regarding functionality, security, design, look / feel and above all, how easy the transition would be.Dashlane pros: offline or online: you only have to be online while register an account, it does not affect working with an already created local database if you choose not to pay orturn off cloud sync secure: 2 factor authentication supported clean interface: identical look / feel on both Windows and OSX security and authorisation: even if you just want to create a brand new local valet (without cloud sync) on a “new” device, it needs to be “approved” with an onlinecode sent to the registered email. Dashlane essentially tracks the devices associated with an account regardless you sync to the cloud or not. browser integration: seamless plugin install and integration during setup auto login: no need to click (most websites) on submit button ever again. If you happen to have multiple accounts for a single site you will have an “drop down” list toselect from. export: csv and its own encrypted .dash format cost: subscription based, annual fee then you can sync your valet to unlimited number of devices including mobile support: good although over email a bit slow security dashboard: gives you warnings about reused passwords, compromised accounts, etc.Dashlane cons: icon view: I hated it, I don’t need a thumbnail to identify an account, no detailed “list like” view. To see any details about any accounts, I had to click on them individually no copy item: I had to fill new entry for few similar items even if 2 fields changed only the interface: designed for web accounts only, not so useful for example PIN numbers where there is no username, it simply won’t save without username field filled import mismatch: The account name and access URLs are really important for autofill, although I had some websites in the valet after import, on visit I got the popupoffering account creation which indicated failed match against my valet stored entry idle lock only: no option to lock on screen lock, etc. although it’s not so much of a concern for an average user import: KeePass import is Windows only, although works sort of well no virtual keyboard: just some extra protection against key-loggers although not so much of a concern main window: over-engineered, password “categories” does not help to organise accounts. Over 100+ accounts this view becomes overwhelming and confusing apache authentication: incapable of filling / recognising apache like authentication popups security: I didn’t like the idea of having the ability to log into their website and browse my password valet (paid only) logo: I just cannot personally associate “antiloop” with password management1Password pros: offline or online: no built in cloud sync option, Google Drive, iCloud, Dropbox, WiFi or just local valet, it’s your choice design: love the application on OSX, similar to KeePass, easy to see details, password strength, etc. in a “detailed like” list view main window: collapsible folder options, excellent organisation and tags, really nice looking interface import: KeePass import allows “field” selection from the CSV export to mach 1Password fields which comes handy for complex entries security: openly published security design trusted: stable software from vendor with good reputation support: great support and community forums export: csv, text or its own .pif format shared vault: could be very handy between family members security audit: reports weak passwords, old or duplicated entries, etc.1Password cons: overpriced: no subscription and copies need to be purchased for multiple devices, mobile is also extra Windows version: may be identical functionality wise but looks / feels totally different to OSX. Ugly, clumsy and suggest very early development. browser plugins: install is not part of the setup, needs to be done manually for each browser from various sources and it’s convoluted / confusing especially on Windows no auto login: no auto field population either, even a simple browser does that. I don’t know why I have to hit ”CMD+&quot; key combination to be able to populate fields andlog in automatically. no virtual keyboard: just some extra protection against key-loggers although not so much of a concern no 2 factor authentication: and it’s not even planned browser plugins based on “websockets” and it can break things or requires specific settings on some platforms , Windows especially apache authentication: incapable of filling / recognising apache like authentication popupsVerdictNo clear winner for me, decided to keep using KeePass until something better comes along. The big shock for me was the apache server authentication, neither of them could handlethose popups so I was left with “cut n’ paste” or “save in the browser”, which was a big deal as I have a lot of those.I liked Dashlane (2.4.1) but it needs refinement, something that 1Password (4.4.1) has on OSX but it is very pricey and I don’t think it’s justified compared to some of the missingfeatures/ annoying bits. I also need at least Windows and OSX support, which is done well from Dashlane but 1Password is very much beta on Windows and I am not even sure, onwhat grounds they can charge money for it.I have not evaluated Lastpass yet, at this stage I simplydo not trust that product and their security practices." }, { "title": "Running Pylint in PyCharm", "url": "/running-pylint-in-pycharm/", "categories": "Python", "tags": "ide, python", "date": "2014-07-03 00:41:28 +0200", "snippet": "I really liked the Pylintintegration in Eclipse/Pydev but I have switched toPyCharm since JetBrains released CE edition. Pycharm supportsPEP8auditing “out of the box”, but I found out lately, that it is a little “loose” on stylecompared to pylint. Running pylint in pycharm didn’t seem to be supported in any ways so I became curious about how I could add this functionality to my favourite IDE.After some searching, I realised that there is not much out there about this topic. I could not accept it and went after the challenge…Running Pylint in PyCharmAs it turned out, it is much simpler than I initially thought. First install pylint, it’s very easy, there are various ways of doing it depending on operating system, yourpreference, etc. so the installation is out of the scope of this guide.Create an external tool:My pylint config is fairly simple, nearly stock standard so I simply added my customisation to the “parameters” line, but for more complex setups, I strongly recommend settingup .pylintrc in your HOME then adding your config file to the parameter line as an argument. This basically pipes your code through pylint and displays the results in the “run”console.Configure output filter:These will create an XML config that looks like this on OSX:$ less Library/Preferences/PyCharm4.0/tools/CodeCompliance.xml&amp;lt;toolSet name=&quot;CodeCompliance&quot;&amp;gt; &amp;lt;tool name=&quot;Pylint&quot; description=&quot;External module to integrate pylint compliance checks into PyCharm&quot; showInMainMenu=&quot;true&quot; showInEditor=&quot;true&quot; showInProject=&quot;true&quot; showInSearchPopup=&quot;true&quot; disabled=&quot;false&quot; useConsole=&quot;true&quot; showConsoleOnStdOut=&quot;false&quot; showConsoleOnStdErr=&quot;false&quot; synchronizeAfterRun=&quot;true&quot;&amp;gt; &amp;lt;exec&amp;gt; &amp;lt;option name=&quot;COMMAND&quot; value=&quot;pylint&quot;/&amp;gt; &amp;lt;option name=&quot;PARAMETERS&quot; value=&quot;--disable=W0106,W0212 --max-line-length=120 --msg-template=&amp;amp;quot;$FileDir$/{path}:{line}: [{msg_id}({symbol}), {obj}] {msg}&amp;amp;quot; $FilePath$&quot;/&amp;gt; &amp;lt;option name=&quot;WORKING_DIRECTORY&quot; value=&quot;$FileDir$&quot;/&amp;gt; &amp;lt;/exec&amp;gt; &amp;lt;filter&amp;gt; &amp;lt;option name=&quot;NAME&quot; value=&quot;ErrorMessages&quot;/&amp;gt; &amp;lt;option name=&quot;DESCRIPTION&quot; value=&quot;Creates jump links to non-compliant lines&quot;/&amp;gt; &amp;lt;option name=&quot;REGEXP&quot; value=&quot;$FILE_PATH$\\:$LINE$\\:&quot;/&amp;gt; &amp;lt;/filter&amp;gt; &amp;lt;/tool&amp;gt;&amp;lt;/toolSet&amp;gt;As the description says, this will essentially give you quick jump links to the parts of your code where the non-compliant code is. Clicking on any of the blue lines will simply navigate you to the line where the issue is, and adding keymap to this tool can improve your workflow too." }, { "title": "Solving Camel ActiveMQ Clients in TCP TIME_WAIT", "url": "/solving-camel-activemq-clients-in-tcp-time_wait/", "categories": "Java", "tags": "activemq, camel, java", "date": "2014-06-04 23:25:27 +0200", "snippet": "We are an agile software development company and agile is great for “moving target”. We plan, work and implement changes in small batches and ongoing re-factoring is justthe nature of what we do.We recently added some functionality as well as increased traffic for one of our Java products utilising Apache Cameland ActiveMQ. The product has been in production for years now, functioning with very much zero defect rate. Not soonafter deploying the new code, our monitoring system triggered alerts about unusually high TCP TIME_WAIT connection states on the server where the new code was running. Webegan the troubleshooting process and found they were all ActiveMQ connections to our broker. Our developers immediately confirmed that“there was no change on the ActiveMQ connection manager side.”Well, it turned out that it was exactly the problem.Solving Camel ActiveMQ Clients in TCP TIME_WAITI started looking various aspects of our environment but was unable to pinpoint where the problem was coming from. So I shifted my focus onto our client implementation, despitethe confirmation from the developers.Note: it’s perfectly natural to have these especially on systems that deal with lots of short lived requests from client connections over unreliable public networks. In nutshell,the local TCP stack waits for twice the maximum segment lifetime (MSL) to pass (120 sec default) before it finishes CLOSING to be sure that the remote end-point received theacknowledgement (and was not queued on upstream routers). Normally it’s harmless, although in large volume could cause memory overflow.We did have enough ephemeral ports to support 3.5K TIME_WAIT sockets, my issue was that it was anextra ~1K and coming from my local network.While looking at our code, I spotted something interesting in our client implementation, we used ActiveMQConnectionFactoryinstead of PooledConnectionFactory. Although ourapplication was functioning, large volume of asynchronous messages created overhead around socket maintenance on server what we don’t need. After replacing our code to usePooledConnectionFactory, we loaded the application into our test environment to confirm the affect.ActiveMQConnectionFactory (old): 24 ESTABLISHED connections to the broker ~150 TIME_WAIT sockets after the initial startup burst of ~1000PooledConnectionFactory (new): 6 ESTABLISHED connections to the broker 0 TIME_WAIT socketsWe managed to reproduce this 100% in our test environment, and deploying the new code into production had not affected our throughput either. Camel, which uses Spring JMS underneath benefits from pooling aware JMS ConnectionFactory such as PooledConnectionFactory hence it is always recommended." }, { "title": "Script to Clone SaltStack Formulas from GitHub", "url": "/script-clone-saltstack-formulas-github/", "categories": "Python", "tags": "python, salt, scripting", "date": "2014-05-23 19:44:01 +0200", "snippet": "I am heavily into Salt infrastructure management at the moment, and wish to leverage all available (community written)formulas. Luckily, the SaltStack group maintains a collection of excellent formulas on their github page,and they are great source for states, ideas, best practices, etc. So I started cloning them, first the ones that I really needed. Then I realized later on, that some I mayneed in the near future so why not clone all of them and ensure I have a local copy of them for my development.The pages have been updated fairly regularly, more and more people contributing now to the project, which is great however it started to become tedious to find new formulasand I needed an automated solution to keep up to date with the changes.Script to Clone SaltStack Formulas from GitHubI could not find anything off-the-shelf, hence I had to come up with my own solution using python.#!/usr/bin/env python&quot;&quot;&quot; Script to find github hosted SaltStack formulas and make a up-to-date local copies of them.&quot;&quot;&quot;import urllib2import reimport osimport subprocess__author__ = &quot;Ivan Vari&quot;&quot;__credits__ = [&quot;Oliver Drake&quot;]__license__ = &quot;GPLv3&quot;__version__ = None__maintainer__ = &quot;Ivan Vari&quot;URL = &quot;https://github.com/saltstack-formulas&quot;PATTERN = &#39;a href=&quot;(/saltstack.*)&quot;\\s&#39;HOME = &#39;{0}/development/saltstack-formulas&#39;.format(os.environ[&#39;HOME&#39;])def main(): &quot;&quot;&quot; Main function and control flow. &quot;&quot;&quot; # print strings with color # http://pythonhosted.org/ANSIColors-balises/ANSIColors.html colorgrn = &quot;\\033[01;32m{0}\\033[00m&quot; colorwht = &quot;\\033[1;37m{0}\\033[00m&quot; # margin for aligned printing width = 50 # default loop controls lastpage = 1 page = 1 # until we reach lastpage while lastpage &amp;gt;= page: page_url = URL + &#39;?page={0}&#39;.format(page) connection = urllib2.Request(page_url) response = urllib2.urlopen(connection) # find all formula-references on the current page for match in re.finditer(PATTERN, response.read()): url = &#39;https://github.com&#39; + match.group(1) # if 1st page, extract last page id and update loop controls if page == 1 and &#39;?page=&#39; in url: ref = re.findall(&#39;page=\\d*&#39;, url) lastpage = int(max(ref).split(&#39;=&#39;)[1]) # then ignore the page id completely if &#39;?page=&#39; not in url: # extract the formula name from the URL formula = url.split(&#39;/&#39;)[-1] repocopy = os.path.join(HOME, formula) # default assumes cloning cmd = &#39;git clone {0} {1}&#39;.format(url, repocopy) exec_dir = &#39;/&#39; # if local copy found, change exec_dir and method to pull if os.path.exists(repocopy): cmd = &#39;git pull&#39;.format(repocopy) exec_dir = repocopy process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE, stderr=subprocess.STDOUT, cwd=exec_dir) # default output definition result = &#39;[OK]&#39; flag = &#39;&#39; # override output definition based on git output message for line in process.stdout.readlines(): if &#39;Cloning&#39; in line: result = &#39;[NEW]&#39; flag = &#39;*&#39; elif &#39;Updating&#39; in line: result = &#39;[UP]&#39; flag = &#39;+&#39; margin = width - (len(result) + len(flag)) print &#39;[{0}]: &#39;.format(formula).ljust(margin, &#39; &#39;) + &#39;=&amp;gt;&#39; + \\ colorwht.format(flag) + colorgrn.format(result) page += 1# boilerplateif __name__ == &quot;__main__&quot;: main()This essentially parses the HTML of the front page and works out the available page numbers then fetches each page individually, filters out all formula references and runs git clone/pull over them depending whether we have local copy of it or not." }, { "title": "Integrating networks over VPN with Amazon VPC", "url": "/integrating-networks-vpn-amazon-vpc/", "categories": "AWS", "tags": "aws, cloud, vpc, ec2", "date": "2014-05-19 00:21:58 +0200", "snippet": "Amazon VPC has been out for some time offering full control of isolated local networking in the cloud.This means that you can have your own private subnet in the cloud, have control over what private IPs your instances are going to use, change the instance type,should your resource requirements increase and so forth.This guide is going to be technical, intended for experienced professionals where I will be discussing options and solutions to securely integrate your onsite(private) LANs with Amazon VPC. It is based on OpenVPN client running on an instance inside VPC, connecting to my remote branch firewall running pfSense 2.1.3and OpenVPN server. The point-to-point tunnel between the client / server is 2-way, both the client and the server expose their local networks and route trafficto the other side accordingly. But first, let’s take a look at what other option we have.Amazon VPC VPNThis is built into VPC and utilizes IPsec which operates on OSI model layer 3 and 4. This technology has been out there for long time, considered fairly secureand supported on many hardware appliances from most well known vendors. Since it operates on lower layer of the OSI model, it’s not intended for end users such asdesktop computers, mobile devices, it’s more suitable for connecting networks aka site-to-site VPN.Features and Benefits Secure. In fact, it was not affected by the recently discovered Heartbleed bug of OpenSSL as it’s usingcrypto built into the kernel. Supports dynamic route management with iBGP, suitable for corporate networks with lots of local LANs. Somewhat easy to implement, widely supported on many hardware platforms, appliances as well as open source software such asOpenswan or strongSwan. Cheap. Currently it’s approximately ~$1 / per day depending on region. It’s really the easiest, simplest, out of box solution at this time of writing.There is a few good guides out there to get you started, however the easiest and most up-to-date is onOpenVPN’s website.VPN terminated on an EC2 Instance inside your VPCWhile it does share some features with IPsec, there are a couple of special ones that apply to OpenVPN: Cost savings. Yes, $1 a day is cheap and I have a supported hardware at my branch endpoint, BUT I already have a requirement to run an instance 24/7 inside my VPCdoing DNS, NTP, git, etc. Adding a VPN and routing functionality on top of those are not hard and just makes sense. I would rather spend that $1 / day on other greatservices such as S3 storage or CloudWatch. Internet availability inside VPC. While the IPsec VPN is superb, it does not provide you internet for your VPC resources, VPC meant to be private, no public trafficin / out. So if I have to have an instance doing SNAT for outgoing Internet inside the VPC, then why not terminate the VPN on the same instance?Build your VPCI will not going into too much detail here, just share the important bits I encountered during my setup. The Amazon guide is well written, please read it for full understandingof the concept. Create a VPC with a preferably /16 mask. eg: 10.0.0.0/16 (Note: CIDR mask must be between /16 and /28) Create at least 2 subnets that overlap with your the VPC CIDR mask, preferably /24. eg: 10.0.0.0/24, 10.0.1.0/24. One will be your private LAN with outgoing only Internet,the other public with in and outgoing Internet. Create a VPC “Internet Gateway”, attach it to your VPC. Ignore “Network ACL” for now, defaults are fine.Pick a private IP address from your public /24 subnet you created in your VPC. eg: 10.0.0.8 This will be the internal address for your EC2 router. While you cannot usestatic private IP addresses inside VPC at this time of writing, you can associate certain addresses to instances. They still use DHCP but always get the same address, mostlikely via MAC address mapping. Associate an “Elastic IP Address” aka public IP for your VPC if you have no free available. Use “network interface” association since we have no instance running yet,and use the private IP you picked above. This essentially creates a “network interface” object and adds your selected public / private IPs to it. Create a DHCP option set (optional) if you want to dish out your own DNS servers, domain names, etc. for your VPC instances, but it’s not required for the VPN routing. Make sure the security group attached to your VPC has port 22 open for 0.0.0.0/0. This is important initially for being able to reach your newly created “router” instancevia the public elastic IP until you sort out the internal routing, firewalling, VPN, etc. You will have a default “route” object attached to your VPC, and it will be set to “Main”. I recommend to leave that untouched which routes only it’s own VPC trafficby default. It’s important for security reasons because “Main” always applies to a subnets that are not associated with any custom “route” object.Then, create a custom “route” object, associate that with your “public” VPC subnet where the VPN instance resides. eg: 10.0.0.0/24. and add your default route to it:0.0.0.0/0          igw-youridThis essentially gives you internet access for outgoing traffic. Networks available inside your VPN EC2 instance (aka networks on the far side of your OpenVPN tunnel) will not be available for any other other instance you launchinto this “public” subnet of your VPC. If you need to reach any of those by their internal VPC IP, you will have to add them to the custom “route” object you associatedwith the “public” subnet as follows192.168.0.0/24         eni-ID10.100.100.0/24        eni-ID10.100.200.0/24        eni-ID10.100.300.0/24        eni-IDThe ID is the “network interface” object you mapped your elastic IP to (attached to your VPN instance). The private class C address is my OpenVPN client network, the other3 are the LANs behind the pfSense VPN server. You also have to remember, that all instances MUST have elastic IP associated to them inside your “public” VPC subnet, theywill not be able to use the VPN router instance!Create another custom “route” object, associate it with your “private” VPC subnet eg: 10.0.1.0/24 and add the following route to it:0.0.0.0/0          eni-IDThis will add default route to all instances for your “private” VPC subnet via the router EC2 instance. This “route” object can be then associated with any other “private”subnet you may create later on and required to support outgoing Internet access.To be able to route any private subnet via your router EC2 instance, you MUST disable source and destination check on the associated network interface object.$ aws --region eu-central-1 ec2 modify-network-interface-attribute --network-interface-id &quot;eni-12f235ec&quot; --no-source-dest-checkPrepare the OpenVPN Server on pfSenseYou will need to create a ccd entry for your VPC EC2 instance to ensure, that the client always gets the same tunnel IP from the server, as well as for the ability to beable to publish the remote networks on the client side to the server. It’s very simple with the web-ui of pfSense, hence I just show the relevant config entry here, shouldyou need to set this on console:ifconfig-push 192.168.0.254 192.168.0.253iroute 10.0.0.0 255.255.255.0iroute 10.0.1.0 255.255.255.0These routes however required to be added to the relevant server config too (if your run multiple). Again, this is easy on web-ui, relevant config entries are:route 10.0.0.0 255.255.255.0route 10.0.1.0 255.255.255.0push &quot;route 10.100.100.0 255.255.255.0&quot;push &quot;route 10.100.200.0 255.255.255.0&quot;push &quot;route 10.100.300.0 255.255.255.0&quot;push &quot;route 10.0.0.0 255.255.255.0&quot;push &quot;route 10.0.1.0 255.255.255.0&quot;Why the redundant route and iroute statements, you might ask? The reason is that route controls the routing from the kernel to the OpenVPN server (via the TUN interface)while iroute controls the routing from the OpenVPN server to the remote clients.And finally, the push-route lines add networks to all VPN clients for all networks available on both side of the tunnel between pfSense and VPC. The reason for this is thatwhen I connect to the pfSense VPN server with my laptop, I want routes set for both, the office and the VPC networks so I can reach everything while away.Launch the Router InstanceThe only thing you need to pay attention to: Ensure that you launch the instance to the correct “public” subnet of your VPC Ensure you set the instance to use the prepared “network interface” objectOnce running, you should be able to reach it via SSH protocol over its public address. Depending on your distro of choice, you may need to turn on routing if not enabled bydefault in the kernel:$ sysctl -w net.ipv4.ip_forward=1OR$ echo 1 &amp;gt; /proc/sys/net/ipv4/ip_forwardInstall then configure your OpenVPN client, set it to run at boot, start, etc. Note: there is nothing specific you need to configure as long as you run client here.Add a couple of iptables rules to NAT traffic coming from your VPC destined to the Internet:$ iptables -A POSTROUTING -s 10.0.0.0/16 -d 10.0.0.0/16 -m comment --comment &quot;VPC-&amp;gt;VPC:SKIP&quot; -j ACCEPT$ iptables -A POSTROUTING -s 10.0.0.0/16 -d 10.100.0.0/16 -m comment --comment &quot;AWS-&amp;gt;REMOTE SUBNETS:SKIP&quot; -j ACCEPT$ iptables -A POSTROUTING -o eth0 -s 10.0.1.0/24 -m comment --comment &quot;SUBNET-PRIVATE-&amp;gt;INTERNET:MASQ&quot; -j SNAT --to-source 10.0.0.8We need the ACCEPT rules to avoid rewriting traffic between VPC subnets as well as between VPC and remote subnets, otherwise they have no affect. Make sure you save therules and enable firewall service to run during boot. Note: this config assumes you have NO other firewall rules configured, those should be added when you have workingrouting.You should be able to reach your instance after all this via it’s private address. When you have a working setup, remove the SSH entry from the “security group” of yourVPC for additional safety." }, { "title": "Couchbase Quota Explained", "url": "/couchbase-quota-explained/", "categories": "NoSQL", "tags": "cluster, couchbase, nosql", "date": "2013-10-16 00:28:13 +0200", "snippet": "For modern, high performance web applications we need low latency and Couchbase excels in that. To maintain the lowest possible latency even during node failure,we need to achieve 100% resident ratio for our high performance buckets. This means that Couchbase serves all your data from RAM, even the least frequently accessed ones,disk is used for persistence only. It turns out that in this condition your usable RAM is lot less, 2 thirds of your allocated quota.I always found it hard to come up with a number for bucket quota. It depends on the key size which can vary, the number of keys expected to be stored, your expiryon keys, etc. So what we do is essentially oversize the buckets initially (as long as we can afford it with RAM of course), then observe and perhaps adjust it later onbut this gets harder when you start utilising your cluster RAM.I had a solid understanding of the “mem_low_wat” and “mem_high_wat” watermarks, I just didn’t know their impact and how their values are calculated.The working set management guideexplains this, I am not going to repeat what is discussed in that, just reflect what happens in real life.In my case the default ratio (per bucket):mem_low_wat = ~60% (of your dynamic RAM quota)mem_high_wat = ~75% (of your dynamic RAM quota) In nutshell: when a certain amount of RAM is consumed by items, the server will eject items starting with replica data. This threshold is known as the low water mark.If higher threshold is breached, Couchbase Server will not only eject replica data, it will also eject less-frequently used items. This ejection happens at node level.I have always monitored both, the resident ratios and of course the quotas but missed the “low” and especially the “high” watermarks. This turned out to be problematicin the last couple of weeks, our cluster lost it’s harmony and we started fetching from disk which adds an extra 100-200us latency to our average GETs occasionally.One of my buckets is somewhat large, currently the quota is set to 100GB (400GB across 4 nodes) and the bucket holds 500+ Million keys. If you consider the ratio above,the default behaviour sets the “mem_high_wat” to 100GB, that is a lot of RAM to be wasted!Explanation:In layman’s term:  it’s a buffer, it’s a protection against unexpected large write spike, node failure, manual failover, rebalance, etc. If the bucket’s memory usagereaches 90% of its quota allocation, the application would start throwing “temporary out of memory errors” which would of course need to be handled some ways.If an application is not coded to handle this type of exception, writes/reads may fail. While these watermarks can be changed on per node / per bucket basis, it’s strongly recommended not to do so as it can potentially put your data at risk." }, { "title": "Couchbase 1.8 Persistence", "url": "/couchbase-1-8-persistence/", "categories": "NoSQL", "tags": "cluster, couchbase, nosql", "date": "2013-05-21 13:34:53 +0200", "snippet": "Couchbase 1.8 supports two types of buckets but the memcached bucket is limited, does notsupport persistence, failover so this article is about the couchbase bucket type and its maintenance.We tend to forget the fact, that this bucket is persisted so every single key is saved to disk. This means you have a copy in memory (assume yourresident ratio is 100%) and on disk. Depending on your cluster setup, you will likely to have at least another copy in another node’s memoryand its disk. (4 copies altogether)With the added metadata overhead, it’s fair to say that you actually need more disk space on each node than memory, to be able to fully utilise yournode’s memory and you have to consider this when you size your hardware. Couchbase 2.x requires even more disk-space (2 x your RAM) per node due tothe JSON indexes and changed persistence layer. The behavior/technique explained here only true up to a certain size, aka vacuum is only practical for smaller databases. For large databases(10G+ per file), it’s much more efficient to fail over the node then add it back to the cluster followed by rebalance.The KeysDepending on what you use a key-value store for, it’s fairly common, that the data you store changes frequently and how long it will remain in yourbucket depends on the expiry you set on creation.You have to nail the expiry to prevent bloating the store or deleting important dataset too earlyWe started loading one of our high performance buckets and set the expiry initially to 30 days. Unfortunately we didn’t realize at that point, that weget more data from web visitors, than we delete (by expiry) so our bucket got bloated with approximately 500 - 600 Million keys. (39G in memory, 45G ondisk per node) This pushed our resident ratio down to 24% approximately, which was not really an issue until we started actually fetching data for ourtesting.The CleanupWe could have raised the bucket quota since we had free RAM, but we knew the bucket was bloated with stale data, so the only option we had was to cleanit up. We set the expiry immediately in our code to 7 days, but it only affected newly created keys so we had to wait 4 weeks before we actually sawour store reducing in size.Our bucket started reducing in size around January but only in memory, the green area representing the disk usage remained the same.By early February we reached our target size, but our disk was still bloated and it had to be claimed back, although it was not as simple as you may think.SQL FragmentationCouchbase 1.8 uses SQLite3 to persist the data, it comes with the product installer package. The knowledgebase(login required) from the support portal explains this very well although in real life scenario, I believe there is a little more to it.The not so Easy WayMake sure you have auto-failover set, remove the node to be cleaned followed by rebalance. When finished add the node back to the cluster and complete rebalance again.When you initiate the rebalance operation the entire Couchbase data area on your disk is deleted, keys and metadata will be synced across all live nodes. Even during off peak production, this is a very heavy operationEvery node will be hitting the disk hard, CPU usage will increase, response times and latency will be poor. One node took approximately 2 x 20 minutes,on our small 4 node cluster it’s 160 minutes and it’s best case scenario. When fully utilized, this could go up as high as 12 hours just to clean our 4 nodecluster and it’s worth to mention that we have the fastest SSD available.The final important aspect of this method is that rebalance fails sometimes, there is a lot going on and it’s somewhat normal according to another article. When it does you have to startthe rebalance all over again, and it could go on for some time until you get all your nodes cleaned. This is something we could not afford, it’s just notfeasible for 24/7 operations.Manual fragmentationThe idea is that you essentially make the data unavailable on a single node until the VACUUM runs then you just suck the data back into memory from thede-fragmented store. Yes, your active-replica will not be activated and in our case (4 node cluster) 25% of our data will not be accessible at all but for usit is better than having uselessly slow 100% while rebalance is running.This requires auto failover to be turned off, you can do it by a simple API call:$ curl &quot;http://localhost:8091/settings/autoFailover&quot; -i -u Administrator:&quot;yourpassword&quot; -d &#39;enabled=false&#39;followed by shutting the coushbase-server process down. When your server process is stopped, you can then VACUUM the database files individually as explainedin the knowledgebase article.The SurpriseIt may fail with the following error:Error: disk I/O errorAfter some reading and digging, we found that SQLite3 makes a copy of your database on the fly to /var/tmp then runs the VACUUM over the temporary copy, on successit copies the de-fragmented database back to its place. Our database files were ~35-40GB each and as much as we pay attention to partitioning when we build servers,we did not count that into our sizing. Not to mention that /var/tmp was not on SSD storage so the VACUUM was slow too, hence we ended up with the following fix forour disk layout:/dev/sdb1 on /opt type ext4 (rw,noatime,data=writeback,commit=120)/opt/temp on /var/tmp type none (rw,bind)Couchbase lives under /opt default so we mounted our fast SSD disk (sdb) there during installation, thus we just added some performance tuning options to our EXT4file system.Then we created a temporary folder at /opt/temp and mounted /var/tmp to it. This way we got enough space to complete the database VACUUM and we moved this highIO operation to our fastest drive available.At completion, start the server process and monitorthe warmup. This method was not only faster, but leaves 75% of our data intact and fast not to mention that we do not have to risk wasting time on rebalance failures.At last but not least re-enable your auto failover when all of your nodes are done:$ curl &quot;http://localhost:8091/settings/autoFailover&quot; -i -u Administrator:&quot;yourpassword&quot; -d &#39;enabled=true&amp;amp;timeout=30&#39;" }, { "title": "Hands-on with Couchbase", "url": "/hands-on-with-couchbase/", "categories": "NoSQL", "tags": "cluster, couchbase, nosql", "date": "2013-05-12 09:42:09 +0200", "snippet": "It’s been a long a long time coming, hard work has finally paid off and the last 7 months feels like just only few weeks.Couchbase is now our primary NoSQL (key-value) store for productionand we are impressed with the results. This article is about our hands-on experience, benchmarking results and its associatedchallenges.BackgroundWe work in the online advertising market and for today’s internet user speed is everything. Therefor, latency is paramount inour application design thus, we needed something fast to store various user information including targeting data.Few years ago, the choice was Voldemort for its latency andspeed, but unfortunately the product was not only vulnerable to cluster changes and disasters, but also was featuring smalluser group so support was difficult. memcached always looked promising but the lack of clustering and disk persistence wastoo expensive for our production suite.Then Couchbase (membase with persistance backend) came along which was pretty new on the market, was going through couple ofre-brandings in a short period but it used memcached under the hood along with seamless clustering, auto-recovery and diskpersistence. Sounds like a dream? Well it was, but we had to wake up quickly in the middle of our migration because it wasjust not going the way we wanted to.The ClusterAs usual, one of our development team grabbed the Java SDK and started working on the code to implement a client in our productionsuite. In the meantime we carved a new cluster out of 4 servers with the following specs: HP DL 360G7 Server 12 cores 256GB RAM 200GB SSD for disk persistence CentOS 6.1 Couchbase Server 1.8.0It appeared to be a capable beast and after a short period, we started populating data to it in parallel with the existing Voldemort stores.We loaded approximately 160 Million, keys to one of our high throughput buckets within a matter of weeks.The FailureInitial tests showed poor latencies of 5ms and our application was throwing zillions of errors within seconds so we started refactoring.We added MBeans to have visibility, gone over the SDK documentation number of times, added debugging metrics and it seemed whatever we do,it is just not working out for us. We began monitoring the servers themselves and noticed that our utilization (context switching, interrupts)do not seem to be affected by our application tests, so I started believing that we trip somewhere so early, that we don’t even make it to the“buckets”.This went on for some time, even engaged the commercial support which was helpful to pinpoint potential issues in our setup, but we have not reallygot much difference in results and our team really exhausted all of its options at that point of time. Couple of months later we felt that it’s timeto look for something else and my team began looking at AeroSpike with the exception of me.I simply refused to give up and was unable to accept failure.The BreakthroughWithout much motivation I continued looking, and after so many hours of troubleshooting, Google searching I bumped into something promising we shouldhave (perhaps) looked at much earlier, the Java Load Test Generatorfrom the community wiki. Disregarding our “sprint” plans, I grabbed a couple of developers along with the code and started a (somewhat) secret projectto work on a “real load tester” application. The code was a bit hard to read but we managed to modify it so it worked with our keys from the pre-populated(160 million) data bucket not with some random generated garbage. We fired it from a single node and immediately managed to squeeze more juice out of thecluster than ever before.This is when the real development commenced with real, production data, on our production suite although we carried out the exercises off peak for ourcustomers’s safety. We had to play with the setup to find the sweet spot and after a day of testing, we finally made a breakthrough. Of course it becameobvious that our client implementation was the problem not the product but without having another implementation alongside and of course enoughhands-on experience, it was impossible to prove. In a couple of weeks time, we completely refactored our client implementation based on the load testerand I am happy to say that it’s in production now for a good couple of months without a single glitch.Benchmark setup Extracted 2 million keys out of our production bucket into a flat file, the load test client read this file into memory during startup Ensured that all of our buckets are 100% memory resident (in-memory store only), even with SSDs reading is just too costly for us Upgraded our cluster to version 1.8.1 and also patched it with Hotfix MB-6550 Vacuumed all of our persisted databases on disk (SQLite) to ensure maximum performance (our production data population, [write only]was constantly running during our benchmarks tests) Copied the modified ycsb.jar along with the rest of the load test code to our NFS share Adjusted the JVM to use 1G Max Heap along with the recommended JVM options however the difference was neglectable Executed the load test from 20 production application servers, all at onceTest configuration:db=com.yahoo.ycsb.db.MembaseClientmemcached.address=192.168.1.1memcached.port=11211histogram.buckets=20exportfile=/tmp/results.txtrecordcount=100000operationcount=2000000workload=com.yahoo.ycsb.workloads.MemcachedCoreWorkloadreadallfields=trueinsertproportion=0readproportion=0.100updateproportion=0scanproportion=0memgetproportion=0.100memupdateproportion=0.0valuelength=2048workingset=100000churndelta=100000printstatsinterval=5requestdistribution=zipfianthreadcount=8JVM tuning:$ java -Xmx1g -XX:+UseConcMarkSweepGC -XX:MaxGCPauseMillis=850 -cp &quot;lib/*:build/*&quot; com.yahoo.ycsb.LoadGenerator -t -P loadtest.cfgResults and Conclusion Couchbase scaled very well during our test, we achieved 400 000 GET requests / second with an average latency of 400us (micro-second), 99th = 4mswithout using Multi-GET CPU usage was not too heavy although we observed 50K to 120K interrupts / second during load testing (per cluster node) Increasing the client side threads beyond 16 doubled the latency with only 20% increase in throughput, although it is likely to be caused by thelimitation of the client application (or its hardware)Results:[OVERALL] RunTime(ms), 30008.0[OVERALL] Throughput(ops/sec), 66648.89362836577[GET] Operations, 2000000[GET] AverageLatency, 401.87us[GET] MinLatency, 146.0us[GET] MaxLatency, 2.11s[GET] 95thPercentileLatency, 2ms   [GET] 99thPercentileLatency, 4ms   [GET] 99.9thPercentileLatency, 16ms  [GET] Return=0, 1989077[GET] Return=-1, 10923[GET] 0us    - 256us , 732757[GET] 256us  - 512us , 1071121[GET] 512us  - 1024us, 159357[GET] 1024us - 2ms   , 22602[GET] 2ms    - 4ms   , 2699[GET] 4ms    - 8ms   , 10831[GET] 8ms    - 16ms  , 403[GET] 16ms   - 32ms  , 38[GET] 32ms   - 64ms  , 0[GET] 64ms   - 128ms , 160[GET] 128ms  - 256ms , 0[GET] 256ms  - 512ms , 0[GET] 512ms  - 1024ms, 0[GET] 1024ms - 2s    , 12[GET] 2s     - 4s    , 20[GET] 4s     - 8s    , 0[GET] 8s     - 16s   , 0[GET] 16s    - 32s   , 0[GET] 32s    - 64s   , 0[GET] 64s    - 128s  , 0[GET] &amp;amp;gt;128s  , 32&amp;lt;/pre&amp;gt;Final VerdictIt seemed that Couchbase really shines between 15% (~75K) and 80% (400K) of it’s max throughput (520K). Low volume buckets (1-2 GETs / second or less)only reach 600 - 700us regardless of disk write queue, resident ratio, etc. This suggests that perhaps there is some kind of prefetch mechanism foractively requested data sets so if you request something frequently, you get better results. Adding more nodes with less memory would increase throughput,improve bucket sharding, reduce network generated interrupts. Couchbase performs better under pressure but you have to keep the pressure within 80% of your maximum throughput to maintain lowest latency." } ]
